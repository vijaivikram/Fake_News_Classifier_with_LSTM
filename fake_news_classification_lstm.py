# -*- coding: utf-8 -*-
"""Fake_News_Classification_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n8B8uCY6pcvSxkdNr2TQW3wexDK9iOhJ
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense,LSTM,Embedding,Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
nltk.download('wordnet')
nltk.download('stopwords')
import re
from nltk.stem import PorterStemmer
ps = PorterStemmer()

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Ineuron/NLP/Fake_News_Classification_LSTM/train.csv')

data.head()

data.isnull().sum()

data.shape

data = data.dropna()

data.isnull().sum()

X = data.drop('label',axis=1)

y = data['label']

messages = X.copy()

#As we have removed the null values, we are resting the index
messages.reset_index(inplace=True)

messages['title'][0]

#here we are considering the title for the classification and we are not considering the text column for classification as it will be computationally very expensive
corpus = []
for i in range(len(messages)):
  review = re.sub('[^a-zA-Z]',' ',messages['title'][i])
  review = review.lower()
  review = review.split()
  review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]
  review = ' '.join(review)
  corpus.append(review)

voc_size=7000

onehot_repr = [one_hot(words,voc_size) for words in corpus]

sent_length = 25
embedded_docs = pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)

embedded_docs[0]

embedding_vector_features = 50

#model Creation
model = Sequential()
model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))
model.add(LSTM(100))
model.add(Dense(1,activation='sigmoid'))

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

X_final = np.array(embedded_docs)
y_final = np.array(y)

X_final.shape,y_final.shape

X_train,X_test,y_train,y_test = train_test_split(X_final,y_final,test_size=0.2)

model.fit(X_final,y_final,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred = (model.predict(X_test) > 0.5).astype("int32")

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,y_pred)

from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)

#Adding Dropout layers
model2 = Sequential()
model2.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))
model2.add(Dropout(0.3))
model2.add(LSTM(100))
model2.add(Dropout(0.3))
model2.add(Dense(1,activation='sigmoid'))

model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model2.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

y_pred2 = (model2.predict(X_test) > 0.5).astype("int32")

confusion_matrix(y_test,y_pred2)

accuracy_score(y_test,y_pred2)